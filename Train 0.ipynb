{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5b7859f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from data_tools.ipynb\n",
      "importing Jupyter notebook from args.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from tensorflow.keras import backend \n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from data_tools import *\n",
    "from args import parser\n",
    "args, unknown = parser.parse_known_args()\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "941ad228",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet(pretrained_weights = None,input_size = (128,128,1)):\n",
    "    size_filter_in = 16\n",
    "    #kernel_init = 'glorot_uniform'\n",
    "    kernel_init=  tf.keras.initializers.GlorotUniform(seed=42)\n",
    "    #kernel_init = 'he_normal'\n",
    "    activation_layer = None \n",
    "    inputs = Input(input_size)\n",
    "    conv1 = Conv2D(size_filter_in, 3, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(inputs)\n",
    "    conv1 = LeakyReLU()(conv1)\n",
    "    conv1 = Conv2D(size_filter_in, 3, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(conv1)\n",
    "    conv1 = LeakyReLU()(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    conv2 = Conv2D(size_filter_in*2, 3, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(pool1)\n",
    "    conv2 = LeakyReLU()(conv2)\n",
    "    conv2 = Conv2D(size_filter_in*2, 3, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(conv2)\n",
    "    conv2 = LeakyReLU()(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    conv3 = Conv2D(size_filter_in*4, 3, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(pool2)\n",
    "    conv3 = LeakyReLU()(conv3)\n",
    "    conv3 = Conv2D(size_filter_in*4, 3, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(conv3)\n",
    "    conv3 = LeakyReLU()(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    conv4 = Conv2D(size_filter_in*8, 3, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(pool3)\n",
    "    conv4 = LeakyReLU()(conv4)\n",
    "    conv4 = Conv2D(size_filter_in*8, 3, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(conv4)\n",
    "    conv4 = LeakyReLU()(conv4)\n",
    "    drop4 = Dropout(0.5)(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "\n",
    "    conv5 = Conv2D(size_filter_in*16, 3, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(pool4)\n",
    "    conv5 = LeakyReLU()(conv5)\n",
    "    conv5 = Conv2D(size_filter_in*16, 3, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(conv5)\n",
    "    conv5 = LeakyReLU()(conv5)\n",
    "    drop5 = Dropout(0.5)(conv5)\n",
    "\n",
    "    up6 = Conv2D(size_filter_in*8, 2, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(UpSampling2D(size = (2,2))(drop5))\n",
    "    up6 = LeakyReLU()(up6)\n",
    "    merge6 = concatenate([drop4,up6], axis = 3)\n",
    "    conv6 = Conv2D(size_filter_in*8, 3, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(merge6)\n",
    "    conv6 = LeakyReLU()(conv6)\n",
    "    conv6 = Conv2D(size_filter_in*8, 3, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(conv6)\n",
    "    conv6 = LeakyReLU()(conv6)\n",
    "    up7 = Conv2D(size_filter_in*4, 2, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(UpSampling2D(size = (2,2))(conv6))\n",
    "    up7 = LeakyReLU()(up7)\n",
    "    merge7 = concatenate([conv3,up7], axis = 3)\n",
    "    conv7 = Conv2D(size_filter_in*4, 3, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(merge7)\n",
    "    conv7 = LeakyReLU()(conv7)\n",
    "    conv7 = Conv2D(size_filter_in*4, 3, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(conv7)\n",
    "    conv7 = LeakyReLU()(conv7)\n",
    "    up8 = Conv2D(size_filter_in*2, 2, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(UpSampling2D(size = (2,2))(conv7))\n",
    "    up8 = LeakyReLU()(up8)\n",
    "    merge8 = concatenate([conv2,up8], axis = 3)\n",
    "    conv8 = Conv2D(size_filter_in*2, 3, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(merge8)\n",
    "    conv8 = LeakyReLU()(conv8)\n",
    "    conv8 = Conv2D(size_filter_in*2, 3, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(conv8)\n",
    "    conv8 = LeakyReLU()(conv8)\n",
    "    \n",
    "    up9 = Conv2D(size_filter_in, 2, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(UpSampling2D(size = (2,2))(conv8))\n",
    "    up9 = LeakyReLU()(up9)\n",
    "    merge9 = concatenate([conv1,up9], axis = 3)\n",
    "    conv9 = Conv2D(size_filter_in, 3, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(merge9)\n",
    "    conv9 = LeakyReLU()(conv9)\n",
    "    conv9 = Conv2D(size_filter_in, 3, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(conv9)\n",
    "    conv9 = LeakyReLU()(conv9)\n",
    "    conv9 = Conv2D(2, 3, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(conv9)\n",
    "    conv9 = LeakyReLU()(conv9)\n",
    "    conv10 = Conv2D(1, 1, activation = 'tanh')(conv9)\n",
    "\n",
    "    model = Model(inputs,conv10)\n",
    "    \n",
    "    model.compile(optimizer = 'adam', loss = tf.keras.losses.Huber(), metrics = ['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74764690",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Path were to read spectrograms of noisy voice and clean voice\n",
    "path_save_spectrogram = args.path_save_spectrogram\n",
    "fig_path= args.fig_dir\n",
    "\n",
    "#path to find pre-trained weights / save models\n",
    "noise_weigth_path= args.noise_weigths_folder\n",
    "voice_weigth_path= args.voice_weigths_folder\n",
    "\n",
    "#pre trained model\n",
    "name_model = args.name_model\n",
    "\n",
    "#Training from scratch vs training from pre-trained weights\n",
    "training_from_scratch = args.training_from_scratch\n",
    "\n",
    "#epochs for training\n",
    "epochs = args.epochs\n",
    "\n",
    "#batch size for training\n",
    "batch_size = args.batch_size\n",
    "\n",
    "SNR= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6350511",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs, batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31b26352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DescribeResult(nobs=393216000, minmax=(array([-34.76199476+0.j]), array([39.37152901+0.j])), mean=array([0.00047881+0.j]), variance=array([0.2401094]), skewness=array([0.1078933+0.j]), kurtosis=array([57.87406579+0.j]))\n",
      "DescribeResult(nobs=393216000, minmax=(array([-26.09865901+0.j]), array([23.34109878+0.j])), mean=array([0.00036607+0.j]), variance=array([0.12240186]), skewness=array([0.14808116+0.j]), kurtosis=array([76.66244846+0.j]))\n",
      "After Scaling\n",
      "DescribeResult(nobs=393216000, minmax=(array([0.2247601+0.j]), array([1.70743058+0.j])), mean=array([0.92000958+0.j]), variance=array([9.604376e-05]), skewness=array([0.1078933+0.j]), kurtosis=array([57.87406579+0.j]))\n",
      "DescribeResult(nobs=393216000, minmax=(array([-0.39144706+0.j]), array([0.21147681+0.j])), mean=array([-0.07316627+0.j]), variance=array([1.82037275e-05]), skewness=array([0.14808116+0.j]), kurtosis=array([76.66244846+0.j]))\n",
      "Train on 21600 samples, validate on 2400 samples\n",
      "Epoch 1/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 2.0815e-05 - mae: 0.0027\n",
      "Epoch 00001: val_loss improved from inf to 0.00001, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 54s 3ms/sample - loss: 2.0793e-05 - mae: 0.0027 - val_loss: 6.8770e-06 - val_mae: 0.0017\n",
      "Epoch 2/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 5.4040e-06 - mae: 0.0015\n",
      "Epoch 00002: val_loss improved from 0.00001 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 48s 2ms/sample - loss: 5.4034e-06 - mae: 0.0015 - val_loss: 4.4999e-06 - val_mae: 0.0014\n",
      "Epoch 3/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 4.1767e-06 - mae: 0.0013\n",
      "Epoch 00003: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 4.1766e-06 - mae: 0.0013 - val_loss: 3.8787e-06 - val_mae: 0.0012\n",
      "Epoch 4/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 3.7690e-06 - mae: 0.0013\n",
      "Epoch 00004: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 46s 2ms/sample - loss: 3.7690e-06 - mae: 0.0013 - val_loss: 3.5649e-06 - val_mae: 0.0012\n",
      "Epoch 5/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 5.3233e-06 - mae: 0.0016\n",
      "Epoch 00005: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 5.3209e-06 - mae: 0.0016 - val_loss: 4.0521e-06 - val_mae: 0.0013\n",
      "Epoch 6/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 3.8052e-06 - mae: 0.0013\n",
      "Epoch 00006: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 3.8049e-06 - mae: 0.0013 - val_loss: 3.6499e-06 - val_mae: 0.0012\n",
      "Epoch 7/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 3.5271e-06 - mae: 0.0012\n",
      "Epoch 00007: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 46s 2ms/sample - loss: 3.5267e-06 - mae: 0.0012 - val_loss: 3.5290e-06 - val_mae: 0.0012\n",
      "Epoch 8/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 3.6424e-06 - mae: 0.0013\n",
      "Epoch 00008: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 3.6417e-06 - mae: 0.0013 - val_loss: 3.8363e-06 - val_mae: 0.0013\n",
      "Epoch 9/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 3.6580e-06 - mae: 0.0013\n",
      "Epoch 00009: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 46s 2ms/sample - loss: 3.6575e-06 - mae: 0.0013 - val_loss: 3.4470e-06 - val_mae: 0.0012\n",
      "Epoch 10/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 3.1198e-06 - mae: 0.0011\n",
      "Epoch 00010: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 47s 2ms/sample - loss: 3.1202e-06 - mae: 0.0011 - val_loss: 2.8121e-06 - val_mae: 0.0011\n",
      "Epoch 11/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 2.6180e-06 - mae: 0.0010\n",
      "Epoch 00011: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 46s 2ms/sample - loss: 2.6177e-06 - mae: 0.0010 - val_loss: 2.4006e-06 - val_mae: 9.7918e-04\n",
      "Epoch 12/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 2.2985e-06 - mae: 9.2351e-04\n",
      "Epoch 00012: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 46s 2ms/sample - loss: 2.2988e-06 - mae: 9.2387e-04 - val_loss: 2.2571e-06 - val_mae: 0.0011\n",
      "Epoch 13/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 2.1034e-06 - mae: 8.7440e-04\n",
      "Epoch 00013: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 46s 2ms/sample - loss: 2.1030e-06 - mae: 8.7435e-04 - val_loss: 2.0116e-06 - val_mae: 8.2938e-04\n",
      "Epoch 14/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 1.9629e-06 - mae: 8.3116e-04\n",
      "Epoch 00014: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 47s 2ms/sample - loss: 1.9628e-06 - mae: 8.3110e-04 - val_loss: 1.9563e-06 - val_mae: 8.0327e-04\n",
      "Epoch 15/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 1.8729e-06 - mae: 8.0584e-04\n",
      "Epoch 00015: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 1.8730e-06 - mae: 8.0584e-04 - val_loss: 1.8044e-06 - val_mae: 7.7354e-04\n",
      "Epoch 16/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 1.7881e-06 - mae: 7.7935e-04\n",
      "Epoch 00016: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 1.7885e-06 - mae: 7.7953e-04 - val_loss: 1.7583e-06 - val_mae: 8.2469e-04\n",
      "Epoch 17/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 1.7303e-06 - mae: 7.6773e-04\n",
      "Epoch 00017: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 46s 2ms/sample - loss: 1.7302e-06 - mae: 7.6768e-04 - val_loss: 1.6616e-06 - val_mae: 7.3648e-04\n",
      "Epoch 18/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 1.6726e-06 - mae: 7.4682e-04\n",
      "Epoch 00018: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 46s 2ms/sample - loss: 1.6724e-06 - mae: 7.4674e-04 - val_loss: 1.5947e-06 - val_mae: 7.2426e-04\n",
      "Epoch 19/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 1.6115e-06 - mae: 7.3553e-04\n",
      "Epoch 00019: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 1.6114e-06 - mae: 7.3547e-04 - val_loss: 1.5723e-06 - val_mae: 7.0889e-04\n",
      "Epoch 20/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 1.5545e-06 - mae: 7.1883e-04\n",
      "Epoch 00020: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 46s 2ms/sample - loss: 1.5546e-06 - mae: 7.1879e-04 - val_loss: 1.5154e-06 - val_mae: 6.9915e-04\n",
      "Epoch 21/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 1.5167e-06 - mae: 7.1096e-04\n",
      "Epoch 00021: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 1.5166e-06 - mae: 7.1093e-04 - val_loss: 1.4964e-06 - val_mae: 6.9311e-04\n",
      "Epoch 22/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21568/21600 [============================>.] - ETA: 0s - loss: 1.4737e-06 - mae: 6.9568e-04\n",
      "Epoch 00022: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 1.4734e-06 - mae: 6.9556e-04 - val_loss: 1.4494e-06 - val_mae: 6.9068e-04\n",
      "Epoch 23/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 1.4380e-06 - mae: 6.8877e-04\n",
      "Epoch 00023: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 1.4381e-06 - mae: 6.8879e-04 - val_loss: 1.4038e-06 - val_mae: 6.6893e-04\n",
      "Epoch 24/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 1.4072e-06 - mae: 6.8047e-04\n",
      "Epoch 00024: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 46s 2ms/sample - loss: 1.4069e-06 - mae: 6.8040e-04 - val_loss: 1.4380e-06 - val_mae: 6.8786e-04\n",
      "Epoch 25/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 1.3837e-06 - mae: 6.7412e-04\n",
      "Epoch 00025: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 46s 2ms/sample - loss: 1.3836e-06 - mae: 6.7409e-04 - val_loss: 1.3547e-06 - val_mae: 6.6974e-04\n",
      "Epoch 26/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 1.3606e-06 - mae: 6.7163e-04\n",
      "Epoch 00026: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 47s 2ms/sample - loss: 1.3604e-06 - mae: 6.7152e-04 - val_loss: 1.3459e-06 - val_mae: 6.5435e-04\n",
      "Epoch 27/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 1.3294e-06 - mae: 6.5687e-04\n",
      "Epoch 00027: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 46s 2ms/sample - loss: 1.3292e-06 - mae: 6.5693e-04 - val_loss: 1.3484e-06 - val_mae: 7.1241e-04\n",
      "Epoch 28/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 1.3130e-06 - mae: 6.5330e-04\n",
      "Epoch 00028: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 46s 2ms/sample - loss: 1.3133e-06 - mae: 6.5338e-04 - val_loss: 1.3048e-06 - val_mae: 6.4750e-04\n",
      "Epoch 29/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 1.3252e-06 - mae: 6.5656e-04\n",
      "Epoch 00029: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 1.3253e-06 - mae: 6.5653e-04 - val_loss: 1.3179e-06 - val_mae: 6.5837e-04\n",
      "Epoch 30/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 1.2801e-06 - mae: 6.4219e-04\n",
      "Epoch 00030: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 46s 2ms/sample - loss: 1.2799e-06 - mae: 6.4210e-04 - val_loss: 1.2600e-06 - val_mae: 6.2957e-04\n",
      "Epoch 31/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 1.2613e-06 - mae: 6.3766e-04\n",
      "Epoch 00031: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 1.2611e-06 - mae: 6.3763e-04 - val_loss: 1.2633e-06 - val_mae: 6.3744e-04\n",
      "Epoch 32/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 1.2540e-06 - mae: 6.3781e-04\n",
      "Epoch 00032: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 46s 2ms/sample - loss: 1.2542e-06 - mae: 6.3784e-04 - val_loss: 1.2496e-06 - val_mae: 6.2997e-04\n",
      "Epoch 33/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 1.2357e-06 - mae: 6.3091e-04\n",
      "Epoch 00033: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 46s 2ms/sample - loss: 1.2356e-06 - mae: 6.3087e-04 - val_loss: 1.2407e-06 - val_mae: 6.2959e-04\n",
      "Epoch 34/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 1.2261e-06 - mae: 6.2768e-04\n",
      "Epoch 00034: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 1.2263e-06 - mae: 6.2766e-04 - val_loss: 1.2148e-06 - val_mae: 6.1798e-04\n",
      "Epoch 35/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 1.2120e-06 - mae: 6.2321e-04\n",
      "Epoch 00035: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 1.2120e-06 - mae: 6.2319e-04 - val_loss: 1.1996e-06 - val_mae: 6.1815e-04\n",
      "Epoch 36/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 1.2059e-06 - mae: 6.2313e-04\n",
      "Epoch 00036: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 1.2057e-06 - mae: 6.2311e-04 - val_loss: 1.1871e-06 - val_mae: 6.0679e-04\n",
      "Epoch 37/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 1.1891e-06 - mae: 6.1312e-04\n",
      "Epoch 00037: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 1.1891e-06 - mae: 6.1309e-04 - val_loss: 1.1902e-06 - val_mae: 6.2104e-04\n",
      "Epoch 38/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 1.1894e-06 - mae: 6.1710e-04\n",
      "Epoch 00038: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 1.1894e-06 - mae: 6.1714e-04 - val_loss: 1.1776e-06 - val_mae: 6.1238e-04\n",
      "Epoch 39/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 1.1725e-06 - mae: 6.1430e-04\n",
      "Epoch 00039: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 46s 2ms/sample - loss: 1.1726e-06 - mae: 6.1433e-04 - val_loss: 1.1758e-06 - val_mae: 6.2026e-04\n",
      "Epoch 40/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 1.1650e-06 - mae: 6.0592e-04\n",
      "Epoch 00040: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 47s 2ms/sample - loss: 1.1648e-06 - mae: 6.0578e-04 - val_loss: 1.1647e-06 - val_mae: 6.0860e-04\n",
      "Epoch 41/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 1.1575e-06 - mae: 6.0704e-04\n",
      "Epoch 00041: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 1.1576e-06 - mae: 6.0709e-04 - val_loss: 1.2062e-06 - val_mae: 6.3205e-04\n",
      "Epoch 42/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 1.1494e-06 - mae: 6.0624e-04\n",
      "Epoch 00042: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 46s 2ms/sample - loss: 1.1493e-06 - mae: 6.0625e-04 - val_loss: 1.1500e-06 - val_mae: 6.0695e-04\n",
      "Epoch 43/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 1.1408e-06 - mae: 5.9972e-04\n",
      "Epoch 00043: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 1.1408e-06 - mae: 5.9978e-04 - val_loss: 1.1665e-06 - val_mae: 6.8089e-04\n",
      "Epoch 44/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 1.1407e-06 - mae: 6.0364e-04\n",
      "Epoch 00044: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 46s 2ms/sample - loss: 1.1408e-06 - mae: 6.0363e-04 - val_loss: 1.1472e-06 - val_mae: 6.1442e-04\n",
      "Epoch 45/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 1.1333e-06 - mae: 6.0081e-04\n",
      "Epoch 00045: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 1.1333e-06 - mae: 6.0078e-04 - val_loss: 1.1370e-06 - val_mae: 5.9681e-04\n",
      "Epoch 46/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21568/21600 [============================>.] - ETA: 0s - loss: 1.1221e-06 - mae: 5.9588e-04\n",
      "Epoch 00046: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 1.1222e-06 - mae: 5.9584e-04 - val_loss: 1.1173e-06 - val_mae: 5.9028e-04\n",
      "Epoch 47/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 1.1176e-06 - mae: 5.9572e-04\n",
      "Epoch 00047: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 1.1175e-06 - mae: 5.9560e-04 - val_loss: 1.1249e-06 - val_mae: 5.9596e-04\n",
      "Epoch 48/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 1.1128e-06 - mae: 5.9519e-04\n",
      "Epoch 00048: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 1.1134e-06 - mae: 5.9531e-04 - val_loss: 1.1052e-06 - val_mae: 5.8490e-04\n",
      "Epoch 49/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 1.1118e-06 - mae: 5.9387e-04\n",
      "Epoch 00049: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 1.1118e-06 - mae: 5.9397e-04 - val_loss: 1.1378e-06 - val_mae: 6.0150e-04\n",
      "Epoch 50/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 1.1087e-06 - mae: 5.9254e-04\n",
      "Epoch 00050: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 1.1088e-06 - mae: 5.9255e-04 - val_loss: 1.1113e-06 - val_mae: 5.9074e-04\n",
      "Epoch 51/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 1.0944e-06 - mae: 5.8768e-04\n",
      "Epoch 00051: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 46s 2ms/sample - loss: 1.0942e-06 - mae: 5.8758e-04 - val_loss: 1.0929e-06 - val_mae: 5.8551e-04\n",
      "Epoch 52/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 1.0923e-06 - mae: 5.8866e-04\n",
      "Epoch 00052: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 1.0924e-06 - mae: 5.8861e-04 - val_loss: 1.1069e-06 - val_mae: 5.9918e-04\n",
      "Epoch 53/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 1.0841e-06 - mae: 5.8514e-04\n",
      "Epoch 00053: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 1.0842e-06 - mae: 5.8517e-04 - val_loss: 1.0962e-06 - val_mae: 5.9748e-04\n",
      "Epoch 54/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 1.0812e-06 - mae: 5.8453e-04\n",
      "Epoch 00054: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 46s 2ms/sample - loss: 1.0810e-06 - mae: 5.8446e-04 - val_loss: 1.0909e-06 - val_mae: 5.8234e-04\n",
      "Epoch 55/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 1.0775e-06 - mae: 5.8340e-04- ETA: 0s - loss: 1.0777e-06 - m\n",
      "Epoch 00055: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 46s 2ms/sample - loss: 1.0774e-06 - mae: 5.8341e-04 - val_loss: 1.0914e-06 - val_mae: 5.8845e-04\n",
      "Epoch 56/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 1.0793e-06 - mae: 5.8420e-04\n",
      "Epoch 00056: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 47s 2ms/sample - loss: 1.0795e-06 - mae: 5.8414e-04 - val_loss: 1.0836e-06 - val_mae: 5.8426e-04\n",
      "Epoch 57/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 1.0704e-06 - mae: 5.8159e-04\n",
      "Epoch 00057: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 47s 2ms/sample - loss: 1.0702e-06 - mae: 5.8155e-04 - val_loss: 1.0753e-06 - val_mae: 5.7892e-04\n",
      "Epoch 58/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 1.0698e-06 - mae: 5.8181e-04\n",
      "Epoch 00058: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 46s 2ms/sample - loss: 1.0701e-06 - mae: 5.8189e-04 - val_loss: 1.0622e-06 - val_mae: 5.7051e-04\n",
      "Epoch 59/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 1.0597e-06 - mae: 5.7745e-04\n",
      "Epoch 00059: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 46s 2ms/sample - loss: 1.0597e-06 - mae: 5.7747e-04 - val_loss: 1.0726e-06 - val_mae: 5.8577e-04\n",
      "Epoch 60/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 1.0580e-06 - mae: 5.7793e-04\n",
      "Epoch 00060: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 1.0583e-06 - mae: 5.7805e-04 - val_loss: 1.0639e-06 - val_mae: 5.8209e-04\n",
      "Epoch 61/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 1.0537e-06 - mae: 5.7656e-04\n",
      "Epoch 00061: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 46s 2ms/sample - loss: 1.0537e-06 - mae: 5.7654e-04 - val_loss: 1.0560e-06 - val_mae: 5.6908e-04\n",
      "Epoch 62/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 1.0497e-06 - mae: 5.7495e-04\n",
      "Epoch 00062: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 1.0495e-06 - mae: 5.7488e-04 - val_loss: 1.0555e-06 - val_mae: 5.8283e-04\n",
      "Epoch 63/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 1.0476e-06 - mae: 5.7449e-04\n",
      "Epoch 00063: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 1.0477e-06 - mae: 5.7451e-04 - val_loss: 1.0597e-06 - val_mae: 5.7156e-04\n",
      "Epoch 64/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 1.0436e-06 - mae: 5.7340e-04\n",
      "Epoch 00064: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 1.0437e-06 - mae: 5.7345e-04 - val_loss: 1.0509e-06 - val_mae: 5.6951e-04\n",
      "Epoch 65/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 1.0398e-06 - mae: 5.7166e-04\n",
      "Epoch 00065: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 1.0395e-06 - mae: 5.7154e-04 - val_loss: 1.0476e-06 - val_mae: 5.7358e-04\n",
      "Epoch 66/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 1.0377e-06 - mae: 5.7262e-04\n",
      "Epoch 00066: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 46s 2ms/sample - loss: 1.0379e-06 - mae: 5.7268e-04 - val_loss: 1.0427e-06 - val_mae: 5.7317e-04\n",
      "Epoch 67/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 1.0338e-06 - mae: 5.6980e-04\n",
      "Epoch 00067: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 1.0338e-06 - mae: 5.6979e-04 - val_loss: 1.0502e-06 - val_mae: 5.9154e-04\n",
      "Epoch 68/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 1.0326e-06 - mae: 5.6976e-04\n",
      "Epoch 00068: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 1.0325e-06 - mae: 5.6974e-04 - val_loss: 1.0498e-06 - val_mae: 5.7523e-04\n",
      "Epoch 69/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 1.0282e-06 - mae: 5.6861e-04\n",
      "Epoch 00069: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 1.0283e-06 - mae: 5.6867e-04 - val_loss: 1.0383e-06 - val_mae: 5.9096e-04\n",
      "Epoch 70/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 1.0247e-06 - mae: 5.6853e-04\n",
      "Epoch 00070: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 1.0245e-06 - mae: 5.6849e-04 - val_loss: 1.0304e-06 - val_mae: 5.7057e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 1.0231e-06 - mae: 5.6725e-04\n",
      "Epoch 00071: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 1.0230e-06 - mae: 5.6718e-04 - val_loss: 1.0384e-06 - val_mae: 5.6684e-04\n",
      "Epoch 72/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 1.0180e-06 - mae: 5.6454e-04\n",
      "Epoch 00072: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 1.0178e-06 - mae: 5.6452e-04 - val_loss: 1.0366e-06 - val_mae: 5.9323e-04\n",
      "Epoch 73/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 1.0187e-06 - mae: 5.6610e-04\n",
      "Epoch 00073: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 1.0185e-06 - mae: 5.6600e-04 - val_loss: 1.0553e-06 - val_mae: 5.9646e-04\n",
      "Epoch 74/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 1.0139e-06 - mae: 5.6400e-04\n",
      "Epoch 00074: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 1.0139e-06 - mae: 5.6402e-04 - val_loss: 1.0202e-06 - val_mae: 5.6485e-04\n",
      "Epoch 75/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 1.0117e-06 - mae: 5.6464e-04- ETA: 1s - loss: 1.0107e-06 - \n",
      "Epoch 00075: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 1.0119e-06 - mae: 5.6475e-04 - val_loss: 1.0258e-06 - val_mae: 5.7590e-04\n",
      "Epoch 76/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 1.0066e-06 - mae: 5.6211e-04\n",
      "Epoch 00076: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 1.0066e-06 - mae: 5.6211e-04 - val_loss: 1.0190e-06 - val_mae: 5.6421e-04\n",
      "Epoch 77/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 1.0066e-06 - mae: 5.6250e-04\n",
      "Epoch 00077: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 1.0066e-06 - mae: 5.6254e-04 - val_loss: 1.0185e-06 - val_mae: 5.6861e-04\n",
      "Epoch 78/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 1.0032e-06 - mae: 5.6080e-04\n",
      "Epoch 00078: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 1.0031e-06 - mae: 5.6083e-04 - val_loss: 1.0150e-06 - val_mae: 5.7403e-04\n",
      "Epoch 79/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 1.0003e-06 - mae: 5.6100e-04\n",
      "Epoch 00079: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 1.0003e-06 - mae: 5.6096e-04 - val_loss: 1.0148e-06 - val_mae: 5.6714e-04\n",
      "Epoch 80/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 9.9917e-07 - mae: 5.5904e-04\n",
      "Epoch 00080: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 9.9919e-07 - mae: 5.5905e-04 - val_loss: 1.0113e-06 - val_mae: 5.7391e-04\n",
      "Epoch 81/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 9.9485e-07 - mae: 5.5871e-04\n",
      "Epoch 00081: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 9.9478e-07 - mae: 5.5872e-04 - val_loss: 1.0058e-06 - val_mae: 5.5823e-04\n",
      "Epoch 82/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 9.9378e-07 - mae: 5.5841e-04\n",
      "Epoch 00082: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 9.9378e-07 - mae: 5.5839e-04 - val_loss: 1.0101e-06 - val_mae: 5.5913e-04\n",
      "Epoch 83/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 9.9102e-07 - mae: 5.5866e-04\n",
      "Epoch 00083: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 9.9114e-07 - mae: 5.5868e-04 - val_loss: 1.0051e-06 - val_mae: 5.5621e-04\n",
      "Epoch 84/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 9.8891e-07 - mae: 5.5583e-04\n",
      "Epoch 00084: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 9.8894e-07 - mae: 5.5589e-04 - val_loss: 1.0065e-06 - val_mae: 5.7263e-04\n",
      "Epoch 85/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 9.8645e-07 - mae: 5.5714e-04\n",
      "Epoch 00085: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 9.8652e-07 - mae: 5.5715e-04 - val_loss: 9.9203e-07 - val_mae: 5.5590e-04\n",
      "Epoch 86/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 9.8479e-07 - mae: 5.5402e-04\n",
      "Epoch 00086: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 9.8462e-07 - mae: 5.5402e-04 - val_loss: 1.0038e-06 - val_mae: 5.8332e-04\n",
      "Epoch 87/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 9.8326e-07 - mae: 5.5457e-04\n",
      "Epoch 00087: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 9.8339e-07 - mae: 5.5460e-04 - val_loss: 9.9383e-07 - val_mae: 5.6611e-04\n",
      "Epoch 88/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 9.7903e-07 - mae: 5.5325e-04\n",
      "Epoch 00088: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 9.7920e-07 - mae: 5.5329e-04 - val_loss: 1.0010e-06 - val_mae: 5.5453e-04\n",
      "Epoch 89/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 9.7887e-07 - mae: 5.5413e-04\n",
      "Epoch 00089: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 9.7934e-07 - mae: 5.5433e-04 - val_loss: 1.1583e-06 - val_mae: 6.8658e-04\n",
      "Epoch 90/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 9.8246e-07 - mae: 5.5659e-04\n",
      "Epoch 00090: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 9.8261e-07 - mae: 5.5661e-04 - val_loss: 9.8946e-07 - val_mae: 5.5506e-04\n",
      "Epoch 91/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 9.7500e-07 - mae: 5.5078e-04\n",
      "Epoch 00091: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 9.7492e-07 - mae: 5.5073e-04 - val_loss: 9.8547e-07 - val_mae: 5.5135e-04\n",
      "Epoch 92/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 9.7103e-07 - mae: 5.5067e-04\n",
      "Epoch 00092: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 9.7140e-07 - mae: 5.5061e-04 - val_loss: 9.8593e-07 - val_mae: 5.5630e-04\n",
      "Epoch 93/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 9.6985e-07 - mae: 5.5106e-04\n",
      "Epoch 00093: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 9.7000e-07 - mae: 5.5109e-04 - val_loss: 9.8471e-07 - val_mae: 5.4716e-04\n",
      "Epoch 94/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 9.6869e-07 - mae: 5.5135e-04\n",
      "Epoch 00094: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 46s 2ms/sample - loss: 9.6875e-07 - mae: 5.5134e-04 - val_loss: 9.8050e-07 - val_mae: 5.6048e-04\n",
      "Epoch 95/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 9.6736e-07 - mae: 5.5055e-04\n",
      "Epoch 00095: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 9.6727e-07 - mae: 5.5056e-04 - val_loss: 9.8126e-07 - val_mae: 5.6477e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 9.6596e-07 - mae: 5.4967e-04\n",
      "Epoch 00096: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 9.6612e-07 - mae: 5.4971e-04 - val_loss: 9.7722e-07 - val_mae: 5.4514e-04\n",
      "Epoch 97/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 9.6332e-07 - mae: 5.4827e-04\n",
      "Epoch 00097: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 9.6313e-07 - mae: 5.4823e-04 - val_loss: 9.7595e-07 - val_mae: 5.5209e-04\n",
      "Epoch 98/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 9.6181e-07 - mae: 5.4891e-04\n",
      "Epoch 00098: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 9.6155e-07 - mae: 5.4887e-04 - val_loss: 9.7553e-07 - val_mae: 5.7608e-04\n",
      "Epoch 99/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 9.6064e-07 - mae: 5.4890e-04\n",
      "Epoch 00099: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 9.6052e-07 - mae: 5.4879e-04 - val_loss: 9.8241e-07 - val_mae: 5.5987e-04\n",
      "Epoch 100/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 9.5691e-07 - mae: 5.4678e-04\n",
      "Epoch 00100: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 9.5698e-07 - mae: 5.4679e-04 - val_loss: 9.6799e-07 - val_mae: 5.4711e-04\n",
      "Epoch 101/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 9.5674e-07 - mae: 5.4740e-04\n",
      "Epoch 00101: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 9.5676e-07 - mae: 5.4739e-04 - val_loss: 9.7504e-07 - val_mae: 5.4869e-04\n",
      "Epoch 102/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 9.5421e-07 - mae: 5.4547e-04\n",
      "Epoch 00102: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 9.5440e-07 - mae: 5.4549e-04 - val_loss: 9.7320e-07 - val_mae: 5.5083e-04\n",
      "Epoch 103/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 9.5545e-07 - mae: 5.4806e-04\n",
      "Epoch 00103: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 44s 2ms/sample - loss: 9.5544e-07 - mae: 5.4803e-04 - val_loss: 9.6810e-07 - val_mae: 5.4813e-04\n",
      "Epoch 104/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 9.5186e-07 - mae: 5.4509e-04\n",
      "Epoch 00104: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 44s 2ms/sample - loss: 9.5188e-07 - mae: 5.4510e-04 - val_loss: 9.7111e-07 - val_mae: 5.5657e-04\n",
      "Epoch 105/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 9.4897e-07 - mae: 5.4425e-04\n",
      "Epoch 00105: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 46s 2ms/sample - loss: 9.4897e-07 - mae: 5.4422e-04 - val_loss: 9.6380e-07 - val_mae: 5.5157e-04\n",
      "Epoch 106/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 9.4691e-07 - mae: 5.4375e-04\n",
      "Epoch 00106: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 46s 2ms/sample - loss: 9.4687e-07 - mae: 5.4372e-04 - val_loss: 9.5673e-07 - val_mae: 5.4526e-04\n",
      "Epoch 107/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 9.4832e-07 - mae: 5.4520e-04\n",
      "Epoch 00107: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 9.4816e-07 - mae: 5.4518e-04 - val_loss: 9.6320e-07 - val_mae: 5.5685e-04\n",
      "Epoch 108/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 9.4548e-07 - mae: 5.4424e-04\n",
      "Epoch 00108: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 9.4539e-07 - mae: 5.4423e-04 - val_loss: 9.7404e-07 - val_mae: 5.4683e-04\n",
      "Epoch 109/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 9.4455e-07 - mae: 5.4371e-04\n",
      "Epoch 00109: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 9.4476e-07 - mae: 5.4380e-04 - val_loss: 9.5816e-07 - val_mae: 5.4745e-04\n",
      "Epoch 110/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 9.4101e-07 - mae: 5.4272e-04\n",
      "Epoch 00110: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 9.4137e-07 - mae: 5.4286e-04 - val_loss: 9.5597e-07 - val_mae: 5.4510e-04\n",
      "Epoch 111/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 9.4099e-07 - mae: 5.4193e-04\n",
      "Epoch 00111: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 9.4107e-07 - mae: 5.4199e-04 - val_loss: 9.5629e-07 - val_mae: 5.5442e-04\n",
      "Epoch 112/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 9.4051e-07 - mae: 5.4251e-04\n",
      "Epoch 00112: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 9.4081e-07 - mae: 5.4256e-04 - val_loss: 9.5437e-07 - val_mae: 5.4226e-04\n",
      "Epoch 113/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 9.4056e-07 - mae: 5.4248e-04\n",
      "Epoch 00113: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 46s 2ms/sample - loss: 9.4046e-07 - mae: 5.4245e-04 - val_loss: 9.5259e-07 - val_mae: 5.4891e-04\n",
      "Epoch 114/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 9.3744e-07 - mae: 5.4176e-04\n",
      "Epoch 00114: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 9.3749e-07 - mae: 5.4171e-04 - val_loss: 9.5671e-07 - val_mae: 5.4019e-04\n",
      "Epoch 115/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 9.3713e-07 - mae: 5.4230e-04\n",
      "Epoch 00115: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 9.3711e-07 - mae: 5.4228e-04 - val_loss: 9.4946e-07 - val_mae: 5.3825e-04\n",
      "Epoch 116/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 9.3499e-07 - mae: 5.4074e-04\n",
      "Epoch 00116: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 9.3484e-07 - mae: 5.4071e-04 - val_loss: 9.5243e-07 - val_mae: 5.3720e-04\n",
      "Epoch 117/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 9.3470e-07 - mae: 5.4028e-04\n",
      "Epoch 00117: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 44s 2ms/sample - loss: 9.3456e-07 - mae: 5.4024e-04 - val_loss: 9.6411e-07 - val_mae: 5.6534e-04\n",
      "Epoch 118/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 9.3135e-07 - mae: 5.3977e-04\n",
      "Epoch 00118: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 9.3148e-07 - mae: 5.3984e-04 - val_loss: 9.5654e-07 - val_mae: 5.4629e-04\n",
      "Epoch 119/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 9.2963e-07 - mae: 5.3798e-04\n",
      "Epoch 00119: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 9.2956e-07 - mae: 5.3796e-04 - val_loss: 9.4897e-07 - val_mae: 5.4522e-04\n",
      "Epoch 120/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 9.3005e-07 - mae: 5.3956e-04\n",
      "Epoch 00120: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 46s 2ms/sample - loss: 9.2997e-07 - mae: 5.3956e-04 - val_loss: 9.4946e-07 - val_mae: 5.5222e-04\n",
      "Epoch 121/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21568/21600 [============================>.] - ETA: 0s - loss: 9.2732e-07 - mae: 5.3790e-04\n",
      "Epoch 00121: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 9.2731e-07 - mae: 5.3786e-04 - val_loss: 9.4735e-07 - val_mae: 5.3695e-04\n",
      "Epoch 122/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 9.2519e-07 - mae: 5.3702e-04\n",
      "Epoch 00122: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 9.2515e-07 - mae: 5.3702e-04 - val_loss: 9.4260e-07 - val_mae: 5.4108e-04\n",
      "Epoch 123/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 9.2522e-07 - mae: 5.3883e-04\n",
      "Epoch 00123: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 9.2540e-07 - mae: 5.3892e-04 - val_loss: 9.4484e-07 - val_mae: 5.4823e-04\n",
      "Epoch 124/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 9.2344e-07 - mae: 5.3571e-04\n",
      "Epoch 00124: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 9.2328e-07 - mae: 5.3567e-04 - val_loss: 9.4189e-07 - val_mae: 5.4060e-04\n",
      "Epoch 125/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 9.2899e-07 - mae: 5.4017e-04\n",
      "Epoch 00125: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 9.2918e-07 - mae: 5.4019e-04 - val_loss: 9.4481e-07 - val_mae: 5.4047e-04\n",
      "Epoch 126/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 9.2254e-07 - mae: 5.3654e-04\n",
      "Epoch 00126: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 9.2262e-07 - mae: 5.3658e-04 - val_loss: 9.4399e-07 - val_mae: 5.6979e-04\n",
      "Epoch 127/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 9.1966e-07 - mae: 5.3483e-04\n",
      "Epoch 00127: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 9.1967e-07 - mae: 5.3478e-04 - val_loss: 9.4698e-07 - val_mae: 5.4613e-04\n",
      "Epoch 128/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 9.1983e-07 - mae: 5.3685e-04- ETA: 2s\n",
      "Epoch 00128: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 46s 2ms/sample - loss: 9.1976e-07 - mae: 5.3683e-04 - val_loss: 9.4269e-07 - val_mae: 5.4337e-04\n",
      "Epoch 129/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 9.1761e-07 - mae: 5.3548e-04\n",
      "Epoch 00129: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 9.1769e-07 - mae: 5.3552e-04 - val_loss: 9.3805e-07 - val_mae: 5.4299e-04\n",
      "Epoch 130/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 9.1651e-07 - mae: 5.3513e-04\n",
      "Epoch 00130: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 46s 2ms/sample - loss: 9.1650e-07 - mae: 5.3514e-04 - val_loss: 9.3485e-07 - val_mae: 5.3298e-04\n",
      "Epoch 131/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 9.1525e-07 - mae: 5.3490e-04\n",
      "Epoch 00131: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 46s 2ms/sample - loss: 9.1522e-07 - mae: 5.3485e-04 - val_loss: 9.3165e-07 - val_mae: 5.3662e-04\n",
      "Epoch 132/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 9.1700e-07 - mae: 5.3535e-04\n",
      "Epoch 00132: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 44s 2ms/sample - loss: 9.1685e-07 - mae: 5.3540e-04 - val_loss: 9.4657e-07 - val_mae: 5.6749e-04\n",
      "Epoch 133/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 9.1361e-07 - mae: 5.3401e-04- ETA: 0s - loss: 9.1319e-07 - mae\n",
      "Epoch 00133: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 9.1339e-07 - mae: 5.3392e-04 - val_loss: 9.3087e-07 - val_mae: 5.4176e-04\n",
      "Epoch 134/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 9.1251e-07 - mae: 5.3359e-04\n",
      "Epoch 00134: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 44s 2ms/sample - loss: 9.1242e-07 - mae: 5.3355e-04 - val_loss: 9.4649e-07 - val_mae: 5.4402e-04\n",
      "Epoch 135/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 9.1297e-07 - mae: 5.3427e-04\n",
      "Epoch 00135: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 9.1283e-07 - mae: 5.3428e-04 - val_loss: 9.4916e-07 - val_mae: 5.6315e-04\n",
      "Epoch 136/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 9.1137e-07 - mae: 5.3374e-04\n",
      "Epoch 00136: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 9.1135e-07 - mae: 5.3373e-04 - val_loss: 9.3176e-07 - val_mae: 5.4478e-04\n",
      "Epoch 137/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 9.0939e-07 - mae: 5.3292e-04\n",
      "Epoch 00137: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 9.0952e-07 - mae: 5.3294e-04 - val_loss: 9.3685e-07 - val_mae: 5.3448e-04\n",
      "Epoch 138/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 9.0903e-07 - mae: 5.3380e-04- ETA: 1s - loss: 9.0876e-07 -\n",
      "Epoch 00138: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 46s 2ms/sample - loss: 9.0908e-07 - mae: 5.3378e-04 - val_loss: 9.2996e-07 - val_mae: 5.4609e-04\n",
      "Epoch 139/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 9.0738e-07 - mae: 5.3186e-04\n",
      "Epoch 00139: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 9.0736e-07 - mae: 5.3185e-04 - val_loss: 9.2948e-07 - val_mae: 5.3037e-04\n",
      "Epoch 140/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 9.0575e-07 - mae: 5.3180e-04\n",
      "Epoch 00140: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 9.0575e-07 - mae: 5.3176e-04 - val_loss: 9.3058e-07 - val_mae: 5.3996e-04\n",
      "Epoch 141/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 9.0519e-07 - mae: 5.3163e-04\n",
      "Epoch 00141: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 46s 2ms/sample - loss: 9.0528e-07 - mae: 5.3167e-04 - val_loss: 9.2751e-07 - val_mae: 5.3942e-04\n",
      "Epoch 142/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 9.0474e-07 - mae: 5.3192e-04\n",
      "Epoch 00142: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 46s 2ms/sample - loss: 9.0481e-07 - mae: 5.3193e-04 - val_loss: 9.2565e-07 - val_mae: 5.3814e-04\n",
      "Epoch 143/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 9.0396e-07 - mae: 5.3163e-04\n",
      "Epoch 00143: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 9.0392e-07 - mae: 5.3165e-04 - val_loss: 9.2794e-07 - val_mae: 5.3872e-04\n",
      "Epoch 144/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 9.0275e-07 - mae: 5.3078e-04\n",
      "Epoch 00144: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 9.0277e-07 - mae: 5.3078e-04 - val_loss: 9.2626e-07 - val_mae: 5.3310e-04\n",
      "Epoch 145/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 9.0190e-07 - mae: 5.3160e-04\n",
      "Epoch 00145: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 9.0188e-07 - mae: 5.3156e-04 - val_loss: 9.2652e-07 - val_mae: 5.3660e-04\n",
      "Epoch 146/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21568/21600 [============================>.] - ETA: 0s - loss: 9.0048e-07 - mae: 5.3035e-04\n",
      "Epoch 00146: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 44s 2ms/sample - loss: 9.0074e-07 - mae: 5.3040e-04 - val_loss: 9.3567e-07 - val_mae: 5.3530e-04\n",
      "Epoch 147/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 9.0198e-07 - mae: 5.3168e-04\n",
      "Epoch 00147: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 44s 2ms/sample - loss: 9.0193e-07 - mae: 5.3168e-04 - val_loss: 9.2670e-07 - val_mae: 5.3664e-04\n",
      "Epoch 148/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 8.9867e-07 - mae: 5.2933e-04\n",
      "Epoch 00148: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 44s 2ms/sample - loss: 8.9861e-07 - mae: 5.2932e-04 - val_loss: 9.2085e-07 - val_mae: 5.3104e-04\n",
      "Epoch 149/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 8.9771e-07 - mae: 5.2952e-04\n",
      "Epoch 00149: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 44s 2ms/sample - loss: 8.9777e-07 - mae: 5.2955e-04 - val_loss: 9.3024e-07 - val_mae: 5.4587e-04\n",
      "Epoch 150/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 8.9729e-07 - mae: 5.2922e-04\n",
      "Epoch 00150: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 44s 2ms/sample - loss: 8.9727e-07 - mae: 5.2924e-04 - val_loss: 9.2123e-07 - val_mae: 5.3022e-04\n",
      "Epoch 151/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 8.9728e-07 - mae: 5.3028e-04\n",
      "Epoch 00151: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 44s 2ms/sample - loss: 8.9736e-07 - mae: 5.3030e-04 - val_loss: 9.2507e-07 - val_mae: 5.3150e-04\n",
      "Epoch 152/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 8.9560e-07 - mae: 5.2936e-04\n",
      "Epoch 00152: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 44s 2ms/sample - loss: 8.9550e-07 - mae: 5.2933e-04 - val_loss: 9.2123e-07 - val_mae: 5.3824e-04\n",
      "Epoch 153/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 8.9568e-07 - mae: 5.2822e-04\n",
      "Epoch 00153: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 44s 2ms/sample - loss: 8.9553e-07 - mae: 5.2821e-04 - val_loss: 9.2596e-07 - val_mae: 5.5473e-04\n",
      "Epoch 154/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 8.9526e-07 - mae: 5.2937e-04\n",
      "Epoch 00154: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 44s 2ms/sample - loss: 8.9535e-07 - mae: 5.2941e-04 - val_loss: 9.1362e-07 - val_mae: 5.2200e-04\n",
      "Epoch 155/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 8.9164e-07 - mae: 5.2658e-04\n",
      "Epoch 00155: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 44s 2ms/sample - loss: 8.9172e-07 - mae: 5.2658e-04 - val_loss: 9.2366e-07 - val_mae: 5.2970e-04\n",
      "Epoch 156/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 8.9209e-07 - mae: 5.2781e-04\n",
      "Epoch 00156: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 44s 2ms/sample - loss: 8.9195e-07 - mae: 5.2781e-04 - val_loss: 9.2932e-07 - val_mae: 5.6299e-04\n",
      "Epoch 157/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 8.9108e-07 - mae: 5.2819e-04\n",
      "Epoch 00157: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 44s 2ms/sample - loss: 8.9146e-07 - mae: 5.2830e-04 - val_loss: 9.1322e-07 - val_mae: 5.2914e-04\n",
      "Epoch 158/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 8.8924e-07 - mae: 5.2591e-04\n",
      "Epoch 00158: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 44s 2ms/sample - loss: 8.8930e-07 - mae: 5.2593e-04 - val_loss: 9.1444e-07 - val_mae: 5.3143e-04\n",
      "Epoch 159/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 8.9036e-07 - mae: 5.2710e-04\n",
      "Epoch 00159: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 8.9018e-07 - mae: 5.2704e-04 - val_loss: 9.1167e-07 - val_mae: 5.2919e-04\n",
      "Epoch 160/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 8.8828e-07 - mae: 5.2739e-04\n",
      "Epoch 00160: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 8.8817e-07 - mae: 5.2739e-04 - val_loss: 9.1722e-07 - val_mae: 5.3504e-04\n",
      "Epoch 161/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 8.8863e-07 - mae: 5.2698e-04\n",
      "Epoch 00161: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 8.8878e-07 - mae: 5.2701e-04 - val_loss: 9.1176e-07 - val_mae: 5.2884e-04\n",
      "Epoch 162/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 8.8827e-07 - mae: 5.2678e-04\n",
      "Epoch 00162: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 8.8836e-07 - mae: 5.2680e-04 - val_loss: 9.1637e-07 - val_mae: 5.3011e-04\n",
      "Epoch 163/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 8.8528e-07 - mae: 5.2530e-04\n",
      "Epoch 00163: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 46s 2ms/sample - loss: 8.8523e-07 - mae: 5.2526e-04 - val_loss: 9.1076e-07 - val_mae: 5.3321e-04\n",
      "Epoch 164/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 8.8745e-07 - mae: 5.2733e-04\n",
      "Epoch 00164: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 8.8765e-07 - mae: 5.2737e-04 - val_loss: 9.0817e-07 - val_mae: 5.2493e-04\n",
      "Epoch 165/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 8.8593e-07 - mae: 5.2593e-04\n",
      "Epoch 00165: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 46s 2ms/sample - loss: 8.8599e-07 - mae: 5.2597e-04 - val_loss: 9.1777e-07 - val_mae: 5.4701e-04\n",
      "Epoch 166/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 8.8492e-07 - mae: 5.2555e-04\n",
      "Epoch 00166: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 46s 2ms/sample - loss: 8.8484e-07 - mae: 5.2552e-04 - val_loss: 9.0423e-07 - val_mae: 5.2434e-04\n",
      "Epoch 167/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 8.8201e-07 - mae: 5.2479e-04\n",
      "Epoch 00167: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 8.8227e-07 - mae: 5.2490e-04 - val_loss: 9.0563e-07 - val_mae: 5.3321e-04\n",
      "Epoch 168/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 8.8160e-07 - mae: 5.2474e-04\n",
      "Epoch 00168: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 8.8174e-07 - mae: 5.2482e-04 - val_loss: 9.0507e-07 - val_mae: 5.2253e-04\n",
      "Epoch 169/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 8.8197e-07 - mae: 5.2571e-04\n",
      "Epoch 00169: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 8.8184e-07 - mae: 5.2569e-04 - val_loss: 9.0890e-07 - val_mae: 5.4137e-04\n",
      "Epoch 170/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 8.8120e-07 - mae: 5.2416e-04\n",
      "Epoch 00170: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 8.8115e-07 - mae: 5.2414e-04 - val_loss: 9.0968e-07 - val_mae: 5.3040e-04\n",
      "Epoch 171/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 8.7977e-07 - mae: 5.2420e-04\n",
      "Epoch 00171: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 8.7974e-07 - mae: 5.2422e-04 - val_loss: 9.0493e-07 - val_mae: 5.2554e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 172/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 8.7901e-07 - mae: 5.2425e-04\n",
      "Epoch 00172: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 8.7908e-07 - mae: 5.2430e-04 - val_loss: 9.0407e-07 - val_mae: 5.2409e-04\n",
      "Epoch 173/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 8.7790e-07 - mae: 5.2326e-04\n",
      "Epoch 00173: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 8.7790e-07 - mae: 5.2326e-04 - val_loss: 9.0729e-07 - val_mae: 5.3947e-04\n",
      "Epoch 174/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 8.7995e-07 - mae: 5.2459e-04\n",
      "Epoch 00174: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 8.8004e-07 - mae: 5.2457e-04 - val_loss: 9.0533e-07 - val_mae: 5.2439e-04\n",
      "Epoch 175/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 8.7762e-07 - mae: 5.2426e-04\n",
      "Epoch 00175: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 8.7769e-07 - mae: 5.2427e-04 - val_loss: 9.1072e-07 - val_mae: 5.3530e-04\n",
      "Epoch 176/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 8.7729e-07 - mae: 5.2398e-04\n",
      "Epoch 00176: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 8.7761e-07 - mae: 5.2400e-04 - val_loss: 9.0080e-07 - val_mae: 5.2478e-04\n",
      "Epoch 177/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 8.7672e-07 - mae: 5.2368e-04\n",
      "Epoch 00177: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 8.7686e-07 - mae: 5.2374e-04 - val_loss: 9.0623e-07 - val_mae: 5.3637e-04\n",
      "Epoch 178/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 8.7477e-07 - mae: 5.2298e-04\n",
      "Epoch 00178: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 8.7470e-07 - mae: 5.2293e-04 - val_loss: 8.9925e-07 - val_mae: 5.2207e-04\n",
      "Epoch 179/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 8.7372e-07 - mae: 5.2252e-04\n",
      "Epoch 00179: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 8.7370e-07 - mae: 5.2254e-04 - val_loss: 9.0163e-07 - val_mae: 5.3630e-04\n",
      "Epoch 180/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 8.7382e-07 - mae: 5.2257e-04\n",
      "Epoch 00180: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 8.7364e-07 - mae: 5.2251e-04 - val_loss: 9.0834e-07 - val_mae: 5.5439e-04\n",
      "Epoch 181/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 8.7303e-07 - mae: 5.2262e-04\n",
      "Epoch 00181: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 8.7337e-07 - mae: 5.2274e-04 - val_loss: 9.0557e-07 - val_mae: 5.3160e-04\n",
      "Epoch 182/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 8.7285e-07 - mae: 5.2273e-04\n",
      "Epoch 00182: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 8.7271e-07 - mae: 5.2271e-04 - val_loss: 8.9656e-07 - val_mae: 5.2996e-04\n",
      "Epoch 183/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 8.7092e-07 - mae: 5.2117e-04\n",
      "Epoch 00183: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 8.7101e-07 - mae: 5.2119e-04 - val_loss: 8.9586e-07 - val_mae: 5.1605e-04\n",
      "Epoch 184/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 8.7088e-07 - mae: 5.2181e-04- ETA: \n",
      "Epoch 00184: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 8.7089e-07 - mae: 5.2182e-04 - val_loss: 9.0647e-07 - val_mae: 5.2769e-04\n",
      "Epoch 185/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 8.7086e-07 - mae: 5.2132e-04- ETA: 1s - loss: 8.7109e-07\n",
      "Epoch 00185: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 8.7090e-07 - mae: 5.2131e-04 - val_loss: 8.9933e-07 - val_mae: 5.3608e-04\n",
      "Epoch 186/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 8.6893e-07 - mae: 5.2156e-04\n",
      "Epoch 00186: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 8.6900e-07 - mae: 5.2158e-04 - val_loss: 9.0572e-07 - val_mae: 5.3010e-04\n",
      "Epoch 187/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 8.6769e-07 - mae: 5.1986e-04\n",
      "Epoch 00187: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 8.6795e-07 - mae: 5.1992e-04 - val_loss: 8.9864e-07 - val_mae: 5.2756e-04\n",
      "Epoch 188/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 8.6802e-07 - mae: 5.2054e-04\n",
      "Epoch 00188: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 8.6805e-07 - mae: 5.2054e-04 - val_loss: 8.9192e-07 - val_mae: 5.2042e-04\n",
      "Epoch 189/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 8.6822e-07 - mae: 5.2150e-04\n",
      "Epoch 00189: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 8.6801e-07 - mae: 5.2144e-04 - val_loss: 9.0431e-07 - val_mae: 5.3767e-04\n",
      "Epoch 190/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 8.6625e-07 - mae: 5.1994e-04\n",
      "Epoch 00190: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 8.6636e-07 - mae: 5.1997e-04 - val_loss: 9.0411e-07 - val_mae: 5.4240e-04\n",
      "Epoch 191/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 8.6604e-07 - mae: 5.2026e-04\n",
      "Epoch 00191: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 8.6597e-07 - mae: 5.2023e-04 - val_loss: 8.9367e-07 - val_mae: 5.2041e-04\n",
      "Epoch 192/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 8.6499e-07 - mae: 5.1988e-04\n",
      "Epoch 00192: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 8.6479e-07 - mae: 5.1982e-04 - val_loss: 8.9496e-07 - val_mae: 5.2119e-04\n",
      "Epoch 193/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 8.6502e-07 - mae: 5.2008e-04\n",
      "Epoch 00193: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 8.6492e-07 - mae: 5.2003e-04 - val_loss: 9.0363e-07 - val_mae: 5.2449e-04\n",
      "Epoch 194/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 8.6528e-07 - mae: 5.1979e-04\n",
      "Epoch 00194: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 8.6526e-07 - mae: 5.1979e-04 - val_loss: 8.9560e-07 - val_mae: 5.2216e-04\n",
      "Epoch 195/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 8.6517e-07 - mae: 5.1987e-04\n",
      "Epoch 00195: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 46s 2ms/sample - loss: 8.6525e-07 - mae: 5.1991e-04 - val_loss: 8.8985e-07 - val_mae: 5.2336e-04\n",
      "Epoch 196/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 8.6344e-07 - mae: 5.1989e-04\n",
      "Epoch 00196: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 8.6341e-07 - mae: 5.1992e-04 - val_loss: 8.9328e-07 - val_mae: 5.2325e-04\n",
      "Epoch 197/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 8.6272e-07 - mae: 5.1868e-04\n",
      "Epoch 00197: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 8.6277e-07 - mae: 5.1871e-04 - val_loss: 8.8871e-07 - val_mae: 5.1772e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 198/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 8.6214e-07 - mae: 5.1904e-04\n",
      "Epoch 00198: val_loss improved from 0.00000 to 0.00000, saving model to ./Speech_Weigths/model_unet= 0.h5\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 8.6187e-07 - mae: 5.1895e-04 - val_loss: 8.8738e-07 - val_mae: 5.1784e-04\n",
      "Epoch 199/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 8.6172e-07 - mae: 5.1904e-04\n",
      "Epoch 00199: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 8.6168e-07 - mae: 5.1903e-04 - val_loss: 8.9664e-07 - val_mae: 5.2529e-04\n",
      "Epoch 200/200\n",
      "21568/21600 [============================>.] - ETA: 0s - loss: 8.6248e-07 - mae: 5.1956e-04\n",
      "Epoch 00200: val_loss did not improve from 0.00000\n",
      "21600/21600 [==============================] - 45s 2ms/sample - loss: 8.6238e-07 - mae: 5.1953e-04 - val_loss: 8.9832e-07 - val_mae: 5.3286e-04\n"
     ]
    }
   ],
   "source": [
    "training_from_scratch = args.training_from_scratch\n",
    "X_in = np.load(path_save_spectrogram +'noisy_concat= '+str(SNR)+\".npy\")\n",
    "X_ou = np.load(path_save_spectrogram +'voice_concat= '+str(SNR)+\".npy\")\n",
    "\n",
    "#Model of noise to predict\n",
    "X_ou = X_in - X_ou\n",
    "\n",
    "#Check distribution\n",
    "print(stats.describe(X_in.reshape(-1,1)))\n",
    "print(stats.describe(X_ou.reshape(-1,1)))\n",
    "\n",
    "#to scale between -1 and 1\n",
    "X_in = scaled_in(X_in)\n",
    "X_ou = scaled_ou(X_ou)\n",
    "\n",
    "#Check new distribution\n",
    "print('After Scaling')\n",
    "print(stats.describe(X_in.reshape(-1,1)))\n",
    "print(stats.describe(X_ou.reshape(-1,1)))\n",
    "\n",
    "#Reshape for training\n",
    "X_in = X_in[:,:,:]\n",
    "X_in = X_in.reshape(X_in.shape[0],X_in.shape[1],X_in.shape[2],1)\n",
    "\n",
    "X_ou = X_ou[:,:,:]\n",
    "X_ou = X_ou.reshape(X_ou.shape[0],X_ou.shape[1],X_ou.shape[2],1)\n",
    "\n",
    "X_train, X_test, y_train, y_test= train_test_split(X_in, X_ou, test_size=0.10, random_state=42)\n",
    "\n",
    "#If training from scratch\n",
    "if training_from_scratch:\n",
    "    generator_nn= unet()\n",
    "\n",
    "#If training from pre-trained weights\n",
    "else:\n",
    "    generator_nn= unet(pretrained_weights = voice_weigth_path+name_model+'= '+str(SNR)+'.h5')\n",
    "\n",
    "#Save best models to disk during training\n",
    "checkpoint = ModelCheckpoint(voice_weigth_path+'/model_unet= '+str(SNR)+'.h5', verbose=1, monitor='val_loss',save_best_only=True, mode='auto')\n",
    "\n",
    "#Training\n",
    "history = generator_nn.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, shuffle=True, callbacks=[checkpoint], verbose=1, validation_data=(X_test, y_test))\n",
    "\n",
    "#Save json file to the disk\n",
    "model_json = generator_nn.to_json()\n",
    "with open(voice_weigth_path+'/' + 'model_unet= '+str(SNR)+'.json', 'w') as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04a3f7b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot the data\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "fig = plt.figure(figsize= (10,5))\n",
    "plt.plot(epochs, loss, label='Training loss')\n",
    "plt.plot(epochs, val_loss, label='Validation loss')\n",
    "plt.yscale('log')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('No. of Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "fig.savefig(fig_path+str(SNR)+'.jpg')\n",
    "fig.savefig(fig_path+str(SNR)+'.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafa5d48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
